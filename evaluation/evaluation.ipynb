{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ecb468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85317527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample manual Q/A data defined in a list of dictionaries\n",
    "qa_data = [\n",
    "    {\n",
    "        \"ID\": 1,\n",
    "        \"Question\": \"What are the key issues with the x-ray bone fracture detection dataset?\",\n",
    "        \"Ground Truth Answer\": \"1) Most of the images are healthy without fracture, 2) In validation set this is opposite, most of the images are with fracture, 3) Enriched by offline augmentation so the number of original images are much smaller, 4) The box sizes tend to be tiny, and the val/test fractures are even smaller than the training fractures.\"\n",
    "    },\n",
    "    {\n",
    "        \"ID\": 2,\n",
    "        \"Question\": \"What are the issues with the annotations in the bone fracture detection dataset?\",\n",
    "        \"Ground Truth Answer\": \"The box sizes tend to be tiny, and the val/test fractures are even smaller than the training fractures. Fractures are annotated with bounding boxes, often covering only a few percent of the image.\"\n",
    "    },\n",
    "    {\n",
    "        \"ID\": 3,\n",
    "        \"Question\": \"What was the impcat of training larger networks on the bone fracture detection dataset?\",\n",
    "        \"Ground Truth Answer\": \"Larger networks did not really help. Their mAP@50 bounced around by a point or two, while the strict mAP@50 to 95 never moved.\"\n",
    "    },\n",
    "    {\n",
    "        \"ID\": 4,\n",
    "        \"Question\": \"What is the bottleneck for Yolo on bone fracture detcteion dataset?\",\n",
    "        \"Ground Truth Answer\": \"The bottleneck is spatial resolution and center point assignment\"\n",
    "    },\n",
    "    {\n",
    "        \"ID\": 5,\n",
    "        \"Question\": \"What should I consider when training yolo on an x-ray bone fracture dataset?\",\n",
    "        \"Ground Truth Answer\": \"1) Check your dataset splits first, 2) Size matters. Small objects require higher resolution, 3) Loss weights help, but they wonâ€™t save you if the detector canâ€™t physically resolve the object, 4) Medical images â‰  street photos. Expect data hunger, imbalance, and tiny targets.\"\n",
    "    }\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0f8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "qa_df = pd.DataFrame(qa_data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"validation_qa_dataset.csv\"\n",
    "qa_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4410c2f",
   "metadata": {},
   "source": [
    "Retrieval Evaluation (ChromaDB/Vector Search Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbfdd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.chains import RetrievalQA\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5c929e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_evaluation(matching_mode='exact',k=1):\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # --- Parameters ---\n",
    "    #matching_mode = \"semantic\"  # Options: \"exact\", \"fuzzy\", \"semantic\"\n",
    "    k=k\n",
    "    fuzzy_threshold = 60\n",
    "    semantic_threshold = 0.80\n",
    "    csv_path = \"validation_qa_dataset.csv\"\n",
    "\n",
    "    # --- Setup Azure OpenAI model and embeddings ---\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=\"2023-05-15\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        model=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=\"2023-05-15\"\n",
    "    )\n",
    "\n",
    "    # --- Connect retriever (Chroma or Azure) ---\n",
    "    vector_db_type = os.getenv(\"VECTOR_DB_TYPE\", \"azure\").lower()\n",
    "\n",
    "    if vector_db_type == \"chroma\":\n",
    "        from langchain.vectorstores import Chroma\n",
    "        retriever = Chroma(persist_directory=\"../chroma_db\", embedding_function=embeddings).as_retriever()\n",
    "    elif vector_db_type == \"azure\":\n",
    "        from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "        retriever = AzureSearch(\n",
    "            azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "            azure_search_key=os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"),\n",
    "            index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "            embedding_function=embeddings.embed_query\n",
    "        ).as_retriever()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported VECTOR_DB_TYPE\")\n",
    "\n",
    "    \n",
    "    retriever.search_kwargs = {\"k\": k}\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "    # --- Matching Functions ---\n",
    "    def is_exact_match(a, b):\n",
    "        return a.strip().lower() == b.strip().lower(), None\n",
    "\n",
    "    def is_fuzzy_match(a, b):\n",
    "        score = fuzz.partial_ratio(a.lower(), b.lower())\n",
    "        return score >= fuzzy_threshold, score\n",
    "\n",
    "    def is_semantic_match(a, b):\n",
    "        a_emb = np.array(embeddings.embed_query(a)).reshape(1, -1)\n",
    "        b_emb = np.array(embeddings.embed_query(b)).reshape(1, -1)\n",
    "        score = cosine_similarity(a_emb, b_emb)[0][0]\n",
    "        return score >= semantic_threshold, score\n",
    "\n",
    "    match_func = {\n",
    "        \"exact\": is_exact_match,\n",
    "        \"fuzzy\": is_fuzzy_match,\n",
    "        \"semantic\": is_semantic_match\n",
    "    }[matching_mode]\n",
    "\n",
    "    # --- Load Evaluation Dataset ---\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = []\n",
    "\n",
    "    # --- Run Evaluation ---\n",
    "    for _, row in df.iterrows():\n",
    "        question = row[\"Question\"]\n",
    "        ground_truth = row[\"Ground Truth Answer\"]\n",
    "        \n",
    "        try:\n",
    "            response = qa_chain.invoke({\"query\": question})\n",
    "            model_answer = response[\"result\"]\n",
    "\n",
    "            is_match, score = match_func(ground_truth, model_answer)\n",
    "            precision = 1.0 if is_match else 0.0\n",
    "            recall = 1.0 if is_match else 0.0\n",
    "\n",
    "            results.append({\n",
    "                \"Question\": question,\n",
    "                \"Ground Truth Answer\": ground_truth,\n",
    "                \"Model Answer\": model_answer,\n",
    "                \"Match\": is_match,\n",
    "                f\"Precision@{k}\": precision,\n",
    "                f\"Recall@{k}\": recall,\n",
    "                \"Score\":round(score, 4)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Question\": question,\n",
    "                \"Ground Truth Answer\": ground_truth,\n",
    "                \"Model Answer\": f\"ERROR: {str(e)}\",\n",
    "                \"Match\": False,\n",
    "                f\"Precision@{k}\": 0.0,\n",
    "                f\"Recall@{k}\": 0.0,\n",
    "                \"Score\":0.0\n",
    "            })\n",
    "\n",
    "    # --- Save & Print Summary ---\n",
    "    result_df = pd.DataFrame(results)\n",
    "    avg_precision = result_df[f\"Precision@{k}\"].mean()\n",
    "    avg_recall = result_df[f\"Recall@{k}\"].mean()\n",
    "    avg_score = result_df[f\"Score\"].mean()\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluation using '{matching_mode}' matching\")\n",
    "    print(f\"ðŸ”¹ Avg Precision@{k}: {avg_precision:.2f}\")\n",
    "    print(f\"ðŸ”¹ Avg Recall@{k}: {avg_recall:.2f}\")\n",
    "    print(f\"ðŸ”¹ Avg Score: {avg_score:.2f}\")\n",
    "\n",
    "    result_df.to_csv(f\"rag_evaluation_{matching_mode}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_evaluation(matching_mode='exact', k=1):\n",
    "    \n",
    "    \n",
    "    load_dotenv()\n",
    "\n",
    "    fuzzy_threshold = 60\n",
    "    semantic_threshold = 0.80\n",
    "    csv_path = \"validation_qa_dataset.csv\"\n",
    "\n",
    "    # Azure OpenAI setup\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=\"2023-05-15\",\n",
    "        temperature=0\n",
    "    )\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        model=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=\"2023-05-15\"\n",
    "    )\n",
    "\n",
    "    # Retriever setup\n",
    "    vector_db_type = os.getenv(\"VECTOR_DB_TYPE\", \"azure\").lower()\n",
    "    if vector_db_type == \"chroma\":\n",
    "        from langchain.vectorstores import Chroma\n",
    "        retriever = Chroma(persist_directory=\"../chroma_db\", embedding_function=embeddings).as_retriever()\n",
    "    elif vector_db_type == \"azure\":\n",
    "        from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "        retriever = AzureSearch(\n",
    "            azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "            azure_search_key=os.getenv(\"AZURE_SEARCH_ADMIN_KEY\"),\n",
    "            index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "            embedding_function=embeddings.embed_query\n",
    "        ).as_retriever()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported VECTOR_DB_TYPE\")\n",
    "\n",
    "    retriever.search_kwargs = {\"k\": k}\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "    # Match functions\n",
    "    def is_exact_match(a, b):\n",
    "        return a.strip().lower() == b.strip().lower(), None\n",
    "\n",
    "    def is_fuzzy_match(a, b):\n",
    "        score = fuzz.partial_ratio(a.lower(), b.lower())\n",
    "        return score >= fuzzy_threshold, score\n",
    "\n",
    "    def is_semantic_match(a, b):\n",
    "        a_emb = np.array(embeddings.embed_query(a)).reshape(1, -1)\n",
    "        b_emb = np.array(embeddings.embed_query(b)).reshape(1, -1)\n",
    "        score = cosine_similarity(a_emb, b_emb)[0][0]\n",
    "        return score >= semantic_threshold, score\n",
    "\n",
    "    def is_gpt_judged_match(question, ground_truth, model_answer):\n",
    "\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "                    You are an expert evaluator. Rate the model's answer on a scale from 1 to 5:\n",
    "\n",
    "                    - 5 = Excellent: Fully correct, complete, and informative.\n",
    "                    - 4 = Good: Mostly correct with minor flaws.\n",
    "                    - 3 = Fair: Partially correct or incomplete.\n",
    "                    - 2 = Poor: Mostly incorrect.\n",
    "                    - 1 = Bad: Completely wrong or irrelevant.\n",
    "\n",
    "                    Question: {question}\n",
    "                    Ground Truth Answer: {ground_truth}\n",
    "                    Model Answer: {model_answer}\n",
    "\n",
    "                    Score (1-5) and brief explanation:\n",
    "                    \"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            reply = response.choices[0].message.content\n",
    "            score_line = next((line for line in reply.splitlines() if any(c.isdigit() for c in line)), \"Score: 3\")\n",
    "            score = int([s for s in score_line if s.isdigit()][0])\n",
    "            explanation = reply.strip()\n",
    "        except Exception as e:\n",
    "            score = 0\n",
    "            explanation = f\"Error: {str(e)}\"\n",
    "\n",
    "        return score >= 4, score, explanation  # score 4â€“5 is considered a match\n",
    "\n",
    "    if matching_mode != \"gpt\":\n",
    "        match_func = {\n",
    "            \"exact\": is_exact_match,\n",
    "            \"fuzzy\": is_fuzzy_match,\n",
    "            \"semantic\": is_semantic_match\n",
    "        }[matching_mode]\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        question = row[\"Question\"]\n",
    "        ground_truth = row[\"Ground Truth Answer\"]\n",
    "        try:\n",
    "            response = qa_chain.invoke({\"query\": question})\n",
    "            model_answer = response[\"result\"]\n",
    "\n",
    "            if matching_mode == \"gpt\":\n",
    "                is_match, score, explanation = is_gpt_judged_match(question, ground_truth, model_answer)\n",
    "            else:\n",
    "                is_match, score = match_func(ground_truth, model_answer)\n",
    "                explanation = \"\"\n",
    "\n",
    "            results.append({\n",
    "                \"Question\": question,\n",
    "                \"Ground Truth Answer\": ground_truth,\n",
    "                \"Model Answer\": model_answer,\n",
    "                \"Match\": is_match,\n",
    "                f\"Precision@{k}\": 1.0 if is_match else 0.0,\n",
    "                f\"Recall@{k}\": 1.0 if is_match else 0.0,\n",
    "                \"Score\": round(score, 4) if isinstance(score, float) else score,\n",
    "                \"Evaluation\": explanation\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Question\": question,\n",
    "                \"Ground Truth Answer\": ground_truth,\n",
    "                \"Model Answer\": f\"ERROR: {str(e)}\",\n",
    "                \"Match\": False,\n",
    "                f\"Precision@{k}\": 0.0,\n",
    "                f\"Recall@{k}\": 0.0,\n",
    "                \"Score\": 0,\n",
    "                \"Evaluation\": f\"Exception during evaluation: {e}\"\n",
    "            })\n",
    "\n",
    "    # Summary\n",
    "    result_df = pd.DataFrame(results)\n",
    "    avg_precision = result_df[f\"Precision@{k}\"].mean()\n",
    "    avg_recall = result_df[f\"Recall@{k}\"].mean()\n",
    "    avg_score = result_df[\"Score\"].mean()\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluation using '{matching_mode}' matching\")\n",
    "    print(f\"ðŸ”¹ Avg Precision@{k}: {avg_precision:.2f}\")\n",
    "    print(f\"ðŸ”¹ Avg Recall@{k}: {avg_recall:.2f}\")\n",
    "    print(f\"ðŸ”¹ Avg Score: {avg_score:.2f}\")\n",
    "\n",
    "    result_df.to_csv(f\"rag_evaluation_{matching_mode}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc8282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "print(\"OPENAI_API_KEY\" in os.environ)\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ffe37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluation using 'gpt' matching\n",
      "ðŸ”¹ Avg Precision@1: 1.00\n",
      "ðŸ”¹ Avg Recall@1: 1.00\n",
      "ðŸ”¹ Avg Score: 4.60\n"
     ]
    }
   ],
   "source": [
    "rag_evaluation(matching_mode='gpt', k=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
